{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('Master': conda)"
  },
  "interpreter": {
   "hash": "57d46a1f3f975f92cc34d815bf69a7d3644582cc16f1cedc66cb95f17202c91e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mne\n",
    "mne.set_log_level(verbose=False)\n",
    "\n",
    "from Datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "channels = ['F4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Omitted 3 annotation(s) that were outside data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Omitted 109 annotation(s) that were outside data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Omitted 92 annotation(s) that were outside data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Omitted 120 annotation(s) that were outside data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Omitted 2 annotation(s) that were outside data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Omitted 104 annotation(s) that were outside data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Limited 2 annotation(s) that were expanding outside the data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Omitted 6 annotation(s) that were outside data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Omitted 2 annotation(s) that were outside data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Omitted 39 annotation(s) that were outside data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Omitted 143 annotation(s) that were outside data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Omitted 234 annotation(s) that were outside data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Limited 3 annotation(s) that were expanding outside the data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Omitted 113 annotation(s) that were outside data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Omitted 4 annotation(s) that were outside data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Omitted 99 annotation(s) that were outside data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "e:\\Python\\TFM\\SSD_IA3\\Notebooks\\Datasets.py:18: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  data = mne.io.read_raw_edf(path)\n",
      "Wall time: 1min\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(14, 14, 11157, 2789)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "%%time\n",
    "datasets = [EDFData_PTH(path, channels=channels) for path in glob(\"../Data/*edf\")]\n",
    "dataset_together = torch.utils.data.ConcatDataset(datasets)\n",
    "dataloaders = [torch.utils.data.DataLoader(dataset, batch_size = BATCH_SIZE, drop_last=True) for dataset in datasets]\n",
    "dataloader_together = torch.utils.data.DataLoader(dataset_together, batch_size = BATCH_SIZE, drop_last=True)\n",
    "len(datasets), len(dataloaders), len(dataset_together), len(dataloader_together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<Epochs |  882 events (all good), 0 - 29.998 sec, baseline off, ~6 kB, data not loaded,\n 'Sleep stage N1': 25\n 'Sleep stage N2': 317\n 'Sleep stage N3': 193\n 'Sleep stage R': 101\n 'Sleep stage W': 246>\n<Epochs |  756 events (all good), 0 - 29.998 sec, baseline off, ~6 kB, data not loaded,\n 'Sleep stage N1': 14\n 'Sleep stage N2': 455\n 'Sleep stage N3': 114\n 'Sleep stage W': 173>\n<Epochs |  715 events (all good), 0 - 29.998 sec, baseline off, ~6 kB, data not loaded,\n 'Sleep stage N1': 4\n 'Sleep stage N2': 248\n 'Sleep stage N3': 185\n 'Sleep stage R': 93\n 'Sleep stage W': 185>\n<Epochs |  764 events (all good), 0 - 29.998 sec, baseline off, ~6 kB, data not loaded,\n 'Sleep stage N1': 14\n 'Sleep stage N2': 403\n 'Sleep stage N3': 218\n 'Sleep stage R': 94\n 'Sleep stage W': 35>\n<Epochs |  805 events (all good), 0 - 29.998 sec, baseline off, ~6 kB, data not loaded,\n 'Sleep stage N1': 127\n 'Sleep stage N2': 293\n 'Sleep stage N3': 169\n 'Sleep stage R': 86\n 'Sleep stage W': 130>\n<Epochs |  780 events (all good), 0 - 29.998 sec, baseline off, ~6 kB, data not loaded,\n 'Sleep stage N1': 67\n 'Sleep stage N2': 448\n 'Sleep stage N3': 37\n 'Sleep stage R': 12\n 'Sleep stage W': 216>\n<Epochs |  840 events (all good), 0 - 29.998 sec, baseline off, ~6 kB, data not loaded,\n 'Sleep stage N1': 63\n 'Sleep stage N2': 388\n 'Sleep stage N3': 235\n 'Sleep stage R': 136\n 'Sleep stage W': 18>\n<Epochs |  853 events (all good), 0 - 29.998 sec, baseline off, ~6 kB, data not loaded,\n 'Sleep stage N1': 145\n 'Sleep stage N2': 365\n 'Sleep stage N3': 219\n 'Sleep stage R': 86\n 'Sleep stage W': 38>\n<Epochs |  762 events (all good), 0 - 29.998 sec, baseline off, ~6 kB, data not loaded,\n 'Sleep stage N1': 35\n 'Sleep stage N2': 334\n 'Sleep stage N3': 209\n 'Sleep stage R': 69\n 'Sleep stage W': 115>\n<Epochs |  738 events (all good), 0 - 29.998 sec, baseline off, ~6 kB, data not loaded,\n 'Sleep stage N1': 85\n 'Sleep stage N2': 370\n 'Sleep stage N3': 146\n 'Sleep stage R': 34\n 'Sleep stage W': 103>\n<Epochs |  813 events (all good), 0 - 29.998 sec, baseline off, ~6 kB, data not loaded,\n 'Sleep stage N1': 143\n 'Sleep stage N2': 392\n 'Sleep stage N3': 76\n 'Sleep stage R': 25\n 'Sleep stage W': 177>\n<Epochs |  760 events (all good), 0 - 29.998 sec, baseline off, ~6 kB, data not loaded,\n 'Sleep stage N1': 13\n 'Sleep stage N2': 405\n 'Sleep stage N3': 94\n 'Sleep stage R': 57\n 'Sleep stage W': 191>\n<Epochs |  853 events (all good), 0 - 29.998 sec, baseline off, ~6 kB, data not loaded,\n 'Sleep stage N1': 15\n 'Sleep stage N2': 308\n 'Sleep stage N3': 419\n 'Sleep stage R': 74\n 'Sleep stage W': 37>\n<Epochs |  836 events (all good), 0 - 29.998 sec, baseline off, ~6 kB, data not loaded,\n 'Sleep stage N1': 36\n 'Sleep stage N2': 208\n 'Sleep stage N3': 286\n 'Sleep stage R': 64\n 'Sleep stage W': 242>\n\n"
     ]
    }
   ],
   "source": [
    "[print(dataset.epochs) for dataset in datasets]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "sampling_rate = int(datasets[0].sampling_rate)\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinySleepNet(nn.Module):\n",
    "    def __init__(self, sampling_rate, channels):\n",
    "        super(TinySleepNet, self).__init__()\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.channels = channels\n",
    "\n",
    "        self.feature_extraction = nn.Sequential(*[\n",
    "            nn.Conv1d(in_channels=len(channels), out_channels=128, kernel_size=sampling_rate//2, stride=sampling_rate//4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=8, stride=8),\n",
    "            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=8, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=8, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=8, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        ])\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, batch_first=True)\n",
    "        \n",
    "        self.classifier = nn.Linear(2*128, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.feature_extraction(X)\n",
    "        X, _ = self.lstm(X.permute(0,2,1))\n",
    "        X = self.classifier(X.reshape(X.shape[0],-1))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TinySleepNet(\n",
       "  (feature_extraction): Sequential(\n",
       "    (0): Conv1d(1, 128, kernel_size=(256,), stride=(128,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(128, 128, kernel_size=(8,), stride=(1,), padding=(3,))\n",
       "    (4): ReLU()\n",
       "    (5): Conv1d(128, 128, kernel_size=(8,), stride=(1,), padding=(3,))\n",
       "    (6): ReLU()\n",
       "    (7): Conv1d(128, 128, kernel_size=(8,), stride=(1,), padding=(3,))\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       "  (classifier): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "model = TinySleepNet(sampling_rate, channels)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, loss_fn, history, X, Y, metrics=None):\n",
    "    \"\"\"\n",
    "    Trains the model over a batch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Torch model.\n",
    "    optimizer:\n",
    "        Torch optimizer.\n",
    "    loss_fn:\n",
    "        Torch loss function.\n",
    "    X: torch.Tensor\n",
    "    Y: torch.Tensor\n",
    "    metrics: dict{str:functions}\n",
    "        Dict of functions (metrics) we want to calculate and their name.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    history: dict{str:float}\n",
    "        Dictionary of metrics.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, Y)\n",
    "    loss.backward()\n",
    "\n",
    "    history['loss'] = loss.item()\n",
    "    if metrics:\n",
    "        with torch.no_grad():\n",
    "            for name, metric_fn in metrics.items():\n",
    "                history[name] = metric_fn(pred, Y)\n",
    "\n",
    "    optimizer.step()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(model, optimizer, loss_fn, history, X, Y, metrics=None):\n",
    "    \"\"\"\n",
    "    Calculates the validation metrics over a batch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Torch model.\n",
    "    optimizer:\n",
    "        Torch optimizer.\n",
    "    loss_fn:\n",
    "        Torch loss function.\n",
    "    X: torch.Tensor\n",
    "    Y: torch.Tensor\n",
    "    metrics: dict{str:functions}\n",
    "        Dict of functions (metrics) we want to calculate and their name.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    history: dict{str:float}\n",
    "        Dictionary of metrics.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, Y)\n",
    "\n",
    "        history['val_loss'] = loss.item()\n",
    "        if metrics:\n",
    "            for name, metric_fn in metrics.items():\n",
    "                history['val_'+name] = metric_fn(pred, Y)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History():\n",
    "    \"\"\"\n",
    "    Class designed to track the metrics during the training of a NN.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.history = {}\n",
    "\n",
    "    def update(self, history):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        history: dict{str:float}\n",
    "        \"\"\"\n",
    "        for metric_name, metric_value in history.items():\n",
    "            if metric_name not in self.history.keys():\n",
    "                self.history[metric_name] = [metric_value]\n",
    "            else:\n",
    "                self.history[metric_name].append(metric_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(Y_pred, Y_true):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of our model given its predictions and labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_pred: torch.Tensor\n",
    "        Raw output from the nn (logits).\n",
    "    Y_true: torch.Tensor\n",
    "        Objective labels.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    accuracy: float\n",
    "    \"\"\"\n",
    "    Y_pred = torch.softmax(Y_pred, dim=-1)\n",
    "    Y_pred = Y_pred.argmax(dim=-1)\n",
    "    accuracy = torch.where(Y_pred==Y_true, 1, 0).sum() / len(Y_true)\n",
    "\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 -> [Train] (Loss) 1.4944 (Acc) 0.4178 [Val] (Loss) 1.4518 (Acc) 0.2331\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-f8c638a471b5>\u001b[0m in \u001b[0;36mval_step\u001b[1;34m(model, optimizer, loss_fn, history, X, Y, metrics)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \"\"\"\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Master\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-66468088d2cb>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Master\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Master\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    582\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    583\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EPOCHS = 5\n",
    "VERBOSE = True\n",
    "\n",
    "metrics = {\n",
    "    'accuracy':accuracy_fn\n",
    "}\n",
    "\n",
    "history_epoch = History()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    history_batch = History()\n",
    "    for batch_i, (X, Y) in enumerate(dataloader_together, 1):\n",
    "        X, Y = X.to(device), torch.squeeze(Y).long().to(device)\n",
    "        history_train = train_step(model, optimizer, loss_fn, {}, X, Y, metrics)\n",
    "        history_batch.update(history_train)\n",
    "        # print(f\"Batch {batch_i} -> Loss: {history.history}\")\n",
    "        # if batch_i == 2:\n",
    "        #     break\n",
    "    for batch_i, (X, Y) in enumerate(dataloader_together, 1):\n",
    "        X, Y = X.to(device), torch.squeeze(Y).long().to(device)\n",
    "        history_val = val_step(model, optimizer, loss_fn, {}, X, Y, metrics)\n",
    "        history_batch.update(history_val)\n",
    "        # if batch_i == 2:\n",
    "        #     break\n",
    "    history_epoch.update({name:np.mean(values) for name,values in history_batch.history.items()})\n",
    "    if VERBOSE:\n",
    "        print(f\"Epoch {epoch+1} -> [Train] (Loss) {history_epoch.history['loss'][-1]:.4f} (Acc) {history_epoch.history['accuracy'][-1]:.4f} | [Val] (Loss) {history_epoch.history['val_loss'][-1]:.4f} (Acc) {history_epoch.history['val_accuracy'][-1]:.4f}\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9669771194458,\n",
       "  1.4156529903411865,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  2.3286783695220947,\n",
       "  1.1198768615722656,\n",
       "  1.1198769807815552,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1153994798660278,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1153993606567383,\n",
       "  1.1019670963287354,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198769807815552,\n",
       "  1.1198768615722656,\n",
       "  1.272242546081543,\n",
       "  1.7293391227722168,\n",
       "  1.648094892501831,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1109219789505005,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1109219789505005,\n",
       "  1.1198768615722656,\n",
       "  1.1198769807815552,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198770999908447,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1064445972442627,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198769807815552,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1109219789505005,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.7915055751800537,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198769807815552,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  1.817668080329895,\n",
       "  1.7293392419815063,\n",
       "  1.729339361190796,\n",
       "  1.7293392419815063,\n",
       "  1.7293392419815063,\n",
       "  2.105191707611084,\n",
       "  2.704530715942383,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1064444780349731,\n",
       "  1.1019669771194458,\n",
       "  1.1019669771194458,\n",
       "  1.1019669771194458,\n",
       "  1.1019669771194458,\n",
       "  1.1019669771194458,\n",
       "  1.1019669771194458,\n",
       "  1.1019669771194458,\n",
       "  1.1019669771194458,\n",
       "  1.1019670963287354,\n",
       "  1.1019669771194458,\n",
       "  1.1019669771194458,\n",
       "  1.1019669771194458,\n",
       "  1.1019669771194458,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.2588101625442505,\n",
       "  2.3286783695220947,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.5769739151000977,\n",
       "  1.729339361190796,\n",
       "  2.105191707611084,\n",
       "  2.1763129234313965,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.8004605770111084,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.11092209815979,\n",
       "  1.1019669771194458,\n",
       "  1.1019669771194458,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019669771194458,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019669771194458,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019669771194458,\n",
       "  1.1019670963287354,\n",
       "  1.11092209815979,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.082655191421509,\n",
       "  2.082655191421509,\n",
       "  1.994326114654541,\n",
       "  1.648094892501831,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.6012661457061768,\n",
       "  2.082655191421509,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.082655191421509,\n",
       "  1.8176684379577637,\n",
       "  2.1763126850128174,\n",
       "  1.6012661457061768,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  2.082655191421509,\n",
       "  2.0826549530029297,\n",
       "  1.648094892501831,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.3605716228485107,\n",
       "  2.082655191421509,\n",
       "  2.0826549530029297,\n",
       "  2.0826549530029297,\n",
       "  1.817668080329895,\n",
       "  1.7293391227722168,\n",
       "  2.481044054031372,\n",
       "  3.2327487468719482,\n",
       "  3.2327487468719482,\n",
       "  2.481043815612793,\n",
       "  2.105191469192505,\n",
       "  2.481043815612793,\n",
       "  2.85689640045166,\n",
       "  1.7293392419815063,\n",
       "  2.105191469192505,\n",
       "  1.7293392419815063,\n",
       "  1.7293392419815063,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  2.105191707611084,\n",
       "  2.85689640045166,\n",
       "  3.2327487468719482,\n",
       "  1.648094892501831,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1153995990753174,\n",
       "  1.1019670963287354,\n",
       "  1.1153995990753174,\n",
       "  2.328678607940674,\n",
       "  1.7293391227722168,\n",
       "  2.105191469192505,\n",
       "  3.2327487468719482,\n",
       "  1.648094892501831,\n",
       "  1.1198768615722656,\n",
       "  1.1198769807815552,\n",
       "  1.1064445972442627,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.2677650451660156,\n",
       "  1.7293391227722168,\n",
       "  2.481044054031372,\n",
       "  1.1198770999908447,\n",
       "  1.4246081113815308,\n",
       "  2.105191707611084,\n",
       "  1.9528260231018066,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.3605716228485107,\n",
       "  2.082655191421509,\n",
       "  2.6577019691467285,\n",
       "  2.370178461074829,\n",
       "  2.082655191421509,\n",
       "  1.8419606685638428,\n",
       "  2.082655191421509,\n",
       "  2.105191469192505,\n",
       "  2.481043815612793,\n",
       "  1.9528260231018066,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1153995990753174,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.2588101625442505,\n",
       "  1.4246079921722412,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.4246081113815308,\n",
       "  1.7293392419815063,\n",
       "  3.2327487468719482,\n",
       "  2.1763129234313965,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198770999908447,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1153993606567383,\n",
       "  1.1019670963287354,\n",
       "  1.1153993606567383,\n",
       "  1.1198768615722656,\n",
       "  1.272242546081543,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  2.481044054031372,\n",
       "  1.7293392419815063,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  3.2327487468719482,\n",
       "  3.2327487468719482,\n",
       "  2.85689640045166,\n",
       "  2.3286783695220947,\n",
       "  1.1198770999908447,\n",
       "  1.1198769807815552,\n",
       "  1.1198769807815552,\n",
       "  1.1198770999908447,\n",
       "  1.1198768615722656,\n",
       "  1.1198769807815552,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1198768615722656,\n",
       "  1.1064445972442627,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1064445972442627,\n",
       "  1.1153993606567383,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.1019670963287354,\n",
       "  1.415653109550476,\n",
       "  1.7293391227722168,\n",
       "  2.85689640045166,\n",
       "  2.704530715942383,\n",
       "  1.1198770999908447,\n",
       "  1.8419606685638428,\n",
       "  2.082655191421509,\n",
       "  2.082655191421509,\n",
       "  2.082655191421509,\n",
       "  2.945225477218628,\n",
       "  2.082655191421509,\n",
       "  2.082655191421509,\n",
       "  2.370178699493408,\n",
       "  3.2327487468719482,\n",
       "  2.6577019691467285,\n",
       "  1.9943263530731201,\n",
       "  2.85689640045166,\n",
       "  2.1763129234313965,\n",
       "  1.1198770999908447,\n",
       "  1.6480950117111206,\n",
       "  3.2327487468719482,\n",
       "  2.1763129234313965,\n",
       "  2.1763129234313965,\n",
       "  2.370178461074829,\n",
       "  2.082655191421509,\n",
       "  2.082655191421509,\n",
       "  2.082655191421509,\n",
       "  2.082655191421509,\n",
       "  2.082655191421509,\n",
       "  2.082655191421509,\n",
       "  2.082655191421509,\n",
       "  2.6577019691467285,\n",
       "  3.2327487468719482,\n",
       "  3.2327487468719482,\n",
       "  2.704530715942383,\n",
       "  1.1198770999908447,\n",
       "  2.704530954360962,\n",
       "  3.2327487468719482,\n",
       "  3.2327487468719482,\n",
       "  2.704530715942383,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.1198770999908447,\n",
       "  1.6480950117111206,\n",
       "  1.7293391227722168,\n",
       "  1.7293392419815063,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  2.105191707611084,\n",
       "  3.2327487468719482,\n",
       "  2.481044054031372,\n",
       "  3.2327487468719482,\n",
       "  1.7293391227722168,\n",
       "  1.7293392419815063,\n",
       "  1.7293392419815063,\n",
       "  2.85689640045166,\n",
       "  3.2327487468719482,\n",
       "  3.2327487468719482,\n",
       "  1.7293391227722168,\n",
       "  1.7293391227722168,\n",
       "  2.481044054031372,\n",
       "  3.2327487468719482,\n",
       "  ...],\n",
       " 'val_accuracy': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.25,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.0,\n",
       "  0.75,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.25,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.25,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.25,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.25,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.75,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.25,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.25,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.75,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.25,\n",
       "  1.0,\n",
       "  0.25,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.75,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.25,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.25,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.75,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.25,\n",
       "  1.0,\n",
       "  0.25,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.75,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.25,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.5,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  ...]}"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "history_batch.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'loss': [1.4943932564491684],\n",
       " 'accuracy': [0.41780207959842236],\n",
       " 'val_loss': [1.4518339147957562],\n",
       " 'val_accuracy': [0.23305844388669775]}"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "history_epoch.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}