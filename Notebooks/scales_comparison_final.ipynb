{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "import re \n",
    "import math\n",
    "from pathlib import Path \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mne\n",
    "mne.set_log_level(verbose=False)\n",
    "import wandb\n",
    "\n",
    "from Datasets import *\n",
    "\n",
    "from utils_folder.callbacks import *\n",
    "from utils_folder.training_pth import *\n",
    "from utils_folder.plotting import *\n",
    "from utils_folder.utils import get_labels_and_preds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_order(file_path):\n",
    "    \"\"\"\n",
    "    Used to order the results from glob, so that the patients are\n",
    "    properly concatenated.\n",
    "    \"\"\"\n",
    "    match = file_pattern.match(Path(file_path).name)\n",
    "    if not match:\n",
    "        return math.inf\n",
    "    return int(match.groups()[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the data from the already prepared ```numpy``` arrays."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Define the regex to sort the files and obtain the paths list\n",
    "file_pattern = re.compile(r'.*?(\\d+).*?')\n",
    "sorted_files = sorted(glob(\"/home/pabloro/master/datos_PSG*.npy\"), key=get_order)\n",
    "### Remove the 10th patient\n",
    "sorted_files_no_10 = [a for a in sorted_files if re.findall(r'\\d+', a)[0]!='10']\n",
    "\n",
    "## Load the data\n",
    "data = [np.load(path_glob) for path_glob in sorted_files_no_10]\n",
    "data = np.concatenate(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Define the regex to sort the files and obtain the paths list\n",
    "file_pattern = re.compile(r'.*?(\\d+).*?')\n",
    "sorted_files = sorted(glob(\"/home/pabloro/master/Etiquetas_PSG*.csv\"), key=get_order)\n",
    "### Remove the 10th patient\n",
    "sorted_files_no_10 = [a for a in sorted_files if re.findall(r'\\d+', a)[0]!='10']\n",
    "\n",
    "## Load the labels\n",
    "labels = [pd.read_csv(path_glob).to_numpy().squeeze() for path_glob in sorted_files_no_10]\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "## Encode the labels\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "labels_encoded = le.transform(labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Separate train-val-test."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "idx_train = np.loadtxt(\"../indices_train.txt\").astype(int)\n",
    "idx_val = np.loadtxt(\"../indices_val.txt\").astype(int)\n",
    "idx_test = np.loadtxt(\"../indices_test_2.txt\").astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, Y_train = data[idx_train], labels_encoded[idx_train]\n",
    "X_val, Y_val = data[idx_val], labels_encoded[idx_val]\n",
    "X_test, Y_test = data[idx_test], labels_encoded[idx_test]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sampling_rate = 512"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_simple_model(sampling_rate, classes):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=sampling_rate//2, strides=sampling_rate//4, activation=\"relu\", input_shape=(15360,1)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_scaled_data(train, val, test):\n",
    "    def _create_scaled_data(data):\n",
    "        ## Create different data\n",
    "        data_std = (data - mean)/std\n",
    "        data_norm = data / np.expand_dims(np.abs(data).max(axis=1),-1)\n",
    "        data_norm_std = (data_norm - data_norm.mean())/data_norm.std()\n",
    "        data_norm_nomean = (data_norm - np.expand_dims(data_norm.mean(axis=1),-1))\n",
    "\n",
    "        ## Put them into a dictionary so its more clean\n",
    "        scaled_data = {\n",
    "            \"Raw Data\":data,\n",
    "            \"Standarized Data\":data_std,\n",
    "            \"Normalized Data\":data_norm,\n",
    "            \"Normalized and Standarized Data\":data_norm_std,\n",
    "            \"Normalized and Mean Substracted Data\":data_norm_nomean\n",
    "        }\n",
    "\n",
    "        return scaled_data\n",
    "\n",
    "    ## Obtain mean and std\n",
    "    mean = train.mean()\n",
    "    std = train.std()\n",
    "\n",
    "    ## Create different data\n",
    "    scaled_train = _create_scaled_data(train)\n",
    "    scaled_val = _create_scaled_data(val)\n",
    "    scaled_test = _create_scaled_data(test)\n",
    "\n",
    "    return scaled_train, scaled_val, scaled_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_scaled_data(scaled_data_train, scaled_data_val, labels_train, labels_val, epochs=50, n_classes=5):\n",
    "    histories = {}\n",
    "\n",
    "    ## Train a model for each different scaled data and store the History object\n",
    "    for name, data in tqdm(scaled_data_train.items()):\n",
    "        data_val = scaled_data_val[name]\n",
    "        model = create_simple_model(512, n_classes)\n",
    "        history = model.fit(np.expand_dims(data, axis=-1), labels_train, batch_size=256, epochs=epochs, verbose=0, \n",
    "                            validation_data=(np.expand_dims(data_val, axis=-1), labels_val))\n",
    "        histories[name] = history\n",
    "    \n",
    "    return histories"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_histories(histories, n_patients):    \n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.suptitle(f\"{n_patients} Patients\")\n",
    "    ## Train\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title(\"Loss\")\n",
    "\n",
    "    for i, (name, history) in enumerate(histories.items()):\n",
    "        plt.plot(history.history[\"loss\"], label=name)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    ## Validation\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title(\"Loss\")\n",
    "\n",
    "    for i, (name, history) in enumerate(histories.items()):\n",
    "        plt.plot(history.history[\"val_loss\"], label=name)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    ## Train\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title(\"Accuracy\")\n",
    "\n",
    "    for i, (name, history) in enumerate(histories.items()):\n",
    "        plt.plot(history.history[\"accuracy\"], label=name)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    # Validation\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.title(\"Accuracy\")\n",
    "\n",
    "    for i, (name, history) in enumerate(histories.items()):\n",
    "        plt.plot(history.history[\"val_accuracy\"], label=name)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_scaled_train, X_scaled_val, X_scaled_test = create_scaled_data(X_train, X_val, X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "histories = train_scaled_data(X_scaled_data_train, X_scaled_data_val, Y_train, Y_val, epochs=50, n_classes=5)\n",
    "plot_histories(histories, len(sorted_files_no_10))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('blyat': conda)"
  },
  "interpreter": {
   "hash": "c815fbc11a24d37b87b0eee41024443fbd64381ad6fcfc6b48c278c2c82a49ea"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}